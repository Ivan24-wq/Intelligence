{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convolutional-intro",
   "metadata": {},
   "source": [
    "# ПРАКТИЧЕСКАЯ РАБОТА №3\n",
    "### _Сверточные нейронные сети для анализа данных_\n",
    "\n",
    "## Теоретическая часть\n",
    "\n",
    "### Введение в сверточные нейронные сети (CNN)\n",
    "\n",
    "**Сверточные нейронные сети** - это специальный тип нейронных сетей, предназначенный для обработки данных с сеточной структурой (изображения, временные ряды, спектрограммы).\n",
    "\n",
    "Основные компоненты CNN:\n",
    "- **Сверточные слои** - извлечение локальных特征\n",
    "- **Пулинговые слои** - уменьшение размерности\n",
    "- **Полносвязные слои** - классификация\n",
    "- **Функции активации** - ReLU, Softmax\n",
    "\n",
    "### Области применения:\n",
    "- Классификация изображений\n",
    "- Обработка временных рядов\n",
    "- Анализ спектральных данных\n",
    "- Распознавание образов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Библиотеки для нейронных сетей\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Настройка отображения\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Практическая часть: Создание CNN для анализа данных ирисов\n",
    "\n",
    "### Задача 1: Подготовка данных для CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-iris-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и анализ данных\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Исходные данные:\")\n",
    "print(f\"Размерность признаков: {X.shape}\")\n",
    "print(f\"Количество классов: {len(target_names)}\")\n",
    "print(f\"Названия классов: {target_names}\")\n",
    "print(f\"Признаки: {feature_names}\")\n",
    "\n",
    "# Создаем DataFrame для анализа\n",
    "df_iris = pd.DataFrame(X, columns=feature_names)\n",
    "df_iris['target'] = y\n",
    "df_iris['species'] = [target_names[i] for i in y]\n",
    "\n",
    "print(\"\\nПервые 5 строк данных:\")\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация распределения данных\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Распределение классов\n",
    "df_iris['species'].value_counts().plot(kind='bar', ax=axes[0,0], color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[0,0].set_title('Распределение видов ирисов')\n",
    "axes[0,0].set_ylabel('Количество')\n",
    "\n",
    "# Корреляционная матрица\n",
    "sns.heatmap(df_iris[feature_names].corr(), annot=True, cmap='coolwarm', ax=axes[0,1])\n",
    "axes[0,1].set_title('Корреляционная матрица признаков')\n",
    "\n",
    "# Boxplot признаков\n",
    "df_iris[feature_names].boxplot(ax=axes[1,0])\n",
    "axes[1,0].set_title('Распределение признаков')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Scatter plot\n",
    "scatter = axes[1,1].scatter(df_iris['petal length (cm)'], df_iris['petal width (cm)'], \n",
    "                           c=df_iris['target'], cmap='viridis')\n",
    "axes[1,1].set_xlabel('Petal Length (cm)')\n",
    "axes[1,1].set_ylabel('Petal Width (cm)')\n",
    "axes[1,1].set_title('Petal Length vs Petal Width')\n",
    "plt.colorbar(scatter, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preprocessing-cnn",
   "metadata": {},
   "source": [
    "### Задача 2: Преобразование данных для CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в формат, подходящий для CNN\n",
    "# Создаем \"псевдо-изображения\" из данных\n",
    "\n",
    "def create_sequence_data(X, sequence_length=4):\n",
    "    \"\"\"Преобразование данных в последовательности для CNN\"\"\"\n",
    "    X_sequence = X.reshape(X.shape[0], sequence_length, 1)\n",
    "    return X_sequence\n",
    "\n",
    "# Преобразуем данные\n",
    "X_sequence = create_sequence_data(X)\n",
    "\n",
    "print(\"До преобразования:\")\n",
    "print(f\"Форма X: {X.shape}\")\n",
    "print(\"\\nПосле преобразования:\")\n",
    "print(f\"Форма X_sequence: {X_sequence.shape}\")\n",
    "print(f\"Пример последовательности для первого образца: {X_sequence[0].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split-cnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequence, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
    "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
    "\n",
    "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape)\n",
    "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape)\n",
    "\n",
    "# Преобразование меток в one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train, num_classes=3)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "print(\"Размерности данных:\")\n",
    "print(f\"X_train: {X_train_scaled.shape}\")\n",
    "print(f\"X_test: {X_test_scaled.shape}\")\n",
    "print(f\"y_train: {y_train_categorical.shape}\")\n",
    "print(f\"y_test: {y_test_categorical.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-architecture",
   "metadata": {},
   "source": [
    "### Задача 3: Создание и обучение сверточной нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-cnn-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание модели CNN\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        # Первый сверточный слой\n",
    "        layers.Conv1D(32, kernel_size=2, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        # Второй сверточный слой\n",
    "        layers.Conv1D(64, kernel_size=2, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        # Выравнивание и полносвязные слои\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Создаем модель\n",
    "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "num_classes = 3\n",
    "\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Компиляция модели\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Архитектура CNN модели:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-cnn-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "history = cnn_model.fit(\n",
    "    X_train_scaled, y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test_scaled, y_test_categorical),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-evaluation",
   "metadata": {},
   "source": [
    "### Задача 4: Оценка качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация процесса обучения\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# График точности\n",
    "axes[0].plot(history.history['accuracy'], label='Обучающая точность')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Валидационная точность')\n",
    "axes[0].set_title('Точность модели')\n",
    "axes[0].set_xlabel('Эпоха')\n",
    "axes[0].set_ylabel('Точность')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# График потерь\n",
    "axes[1].plot(history.history['loss'], label='Обучающие потери')\n",
    "axes[1].plot(history.history['val_loss'], label='Валидационные потери')\n",
    "axes[1].set_title('Потери модели')\n",
    "axes[1].set_xlabel('Эпоха')\n",
    "axes[1].set_ylabel('Потери')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания и оценка модели\n",
    "y_pred_proba = cnn_model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Метрики качества\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Точность CNN модели: {accuracy:.3f}\")\n",
    "\n",
    "print(\"\\nОтчет по классификации:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix-cnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Матрица ошибок\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names,\n",
    "            yticklabels=target_names)\n",
    "plt.title('Матрица ошибок CNN модели')\n",
    "plt.ylabel('Истинные значения')\n",
    "plt.xlabel('Предсказанные значения')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-with-dnn",
   "metadata": {},
   "source": [
    "### Задача 5: Сравнение с полносвязной нейронной сетью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dnn-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание полносвязной нейронной сети для сравнения\n",
    "def create_dnn_model(input_dim, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Подготовка данных для DNN\n",
    "X_train_dnn = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "X_test_dnn = X_test_scaled.reshape(X_test_scaled.shape[0], -1)\n",
    "\n",
    "# Создание и обучение DNN\n",
    "dnn_model = create_dnn_model(X_train_dnn.shape[1], num_classes)\n",
    "dnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_dnn = dnn_model.fit(\n",
    "    X_train_dnn, y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test_dnn, y_test_categorical),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Предсказания DNN\n",
    "y_pred_dnn = np.argmax(dnn_model.predict(X_test_dnn), axis=1)\n",
    "accuracy_dnn = accuracy_score(y_test, y_pred_dnn)\n",
    "\n",
    "print(\"СРАВНЕНИЕ МОДЕЛЕЙ:\")\n",
    "print(f\"Точность CNN: {accuracy:.3f}\")\n",
    "print(f\"Точность DNN: {accuracy_dnn:.3f}\")\n",
    "\n",
    "# Сравнение архитектур\n",
    "print(\"\\nСРАВНЕНИЕ АРХИТЕКТУР:\")\n",
    "print(\"CNN - количество параметров:\", cnn_model.count_params())\n",
    "print(\"DNN - количество параметров:\", dnn_model.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-section",
   "metadata": {},
   "source": [
    "# ЗАДАНИЕ ДЛЯ ВЫПОЛНЕНИЯ\n",
    "\n",
    "### Тема: \"Сверточные нейронные сети для анализа различных типов данных\"\n",
    "\n",
    "**Цель:** Исследовать применение CNN для анализа разных типов данных и сравнить эффективность различных архитектур.\n",
    "\n",
    "### Задание 1: Анализ временных рядов с помощью CNN\n",
    "\n",
    "**Задача:** Использовать CNN для классификации временных рядов на примере датасета ECG\n",
    "\n",
    "**Подзадачи:**\n",
    "1. Загрузите датасет временных рядов (согласно варианту)\n",
    "2. Подготовьте данные для CNN (нормализация, создание последовательностей)\n",
    "3. Создайте CNN архитектуру для анализа временных рядов\n",
    "4. Обучите модель и оцените ее качество\n",
    "5. Сравните с другими методами классификации временных рядов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assignment-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Загрузка данных\n",
    "CSV_FILE = \"train.csv\"\n",
    "data = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Обработка пропусков\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Кодировка\n",
    "data['Sex'] = LabelEncoder().fit_transform(data['Sex'])\n",
    "data['Embarked'] = LabelEncoder().fit_transform(data['Embarked'])\n",
    "\n",
    "# Признаки\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X = data[features].values\n",
    "y = data['Survived'].values\n",
    "\n",
    "# Нормализация\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# reshape для Conv1D\n",
    "X_seq = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "# train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=62)\n",
    "\n",
    "# CNN модель\n",
    "model = Sequential([\n",
    "    Conv1D(32, 2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Сравнение с классическими методами\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], X_train.shape[1])\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(X_train_flat, y_train)\n",
    "print(\"LogReg Accuracy:\", accuracy_score(y_test, lr.predict(X_test_flat)))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_flat, y_train)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf.predict(X_test_flat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-2-description",
   "metadata": {},
   "source": [
    "### Задание 2: CNN для многомерных данных\n",
    "\n",
    "**Задача:** Разработать CNN для анализа многомерных данных из выбранного датасета\n",
    "\n",
    "**Подзадачи:**\n",
    "1. Выберите многомерный датасет (согласно варианту)\n",
    "2. Преобразуйте данные в формат, подходящий для CNN (2D или 3D структуры)\n",
    "3. Создайте и обучите несколько архитектур CNN\n",
    "4. Проведите оптимизацию гиперпараметров\n",
    "5. Проанализируйте влияние глубины сети на качество классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assignment-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. ЗАГРУЗКА ДАТАСЕТА\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(\"Размеры:\", X_train.shape, y_train.shape)   # (60000, 28, 28)\n",
    "\n",
    "# 2. ПРЕОБРАЗОВАНИЕ ДАННЫХ ДЛЯ CNN\n",
    "\n",
    "# Добавление канала (28, 28) -> (28, 28, 1)\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\")  / 255.0\n",
    "\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test  = np.expand_dims(X_test, -1)\n",
    "\n",
    "# Разделяем на train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_oh = to_categorical(y_train, 10)\n",
    "y_val_oh   = to_categorical(y_val,   10)\n",
    "y_test_oh  = to_categorical(y_test,  10)\n",
    "\n",
    "# 3. СОЗДАНИЕ НЕСКОЛЬКИХ АРХИТЕКТУР CNN\n",
    "\n",
    "def build_cnn_model(depth=2, filters=32):\n",
    "    \"\"\"\n",
    "    depth — количество блоков Conv+ReLU+Pool\n",
    "    filters — базовое число фильтров\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(28, 28, 1)))\n",
    "\n",
    "    for i in range(depth):\n",
    "        model.add(layers.Conv2D(filters * (2**i), (3,3), activation='relu', padding=\"same\"))\n",
    "        model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Пример трёх моделей разной сложности\n",
    "models_to_train = {\n",
    "    \"CNN_small\":  build_cnn_model(depth=1, filters=16),\n",
    "    \"CNN_medium\": build_cnn_model(depth=2, filters=32),\n",
    "    \"CNN_large\":  build_cnn_model(depth=3, filters=32),\n",
    "}\n",
    "\n",
    "\n",
    "history_dict = {}\n",
    "\n",
    "for name, model in models_to_train.items():\n",
    "    print(f\"\\n==== Обучение модели {name} ====\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train_oh,\n",
    "        validation_data=(X_val, y_val_oh),\n",
    "        epochs=5, batch_size=64, verbose=1\n",
    "    )\n",
    "    history_dict[name] = history\n",
    "\n",
    "# 4. ПРОСТАЯ ОПТИМИЗАЦИЯ ГИПЕРПАРАМЕТРОВ\n",
    "def grid_search():\n",
    "    depths = [1, 2, 3]\n",
    "    filters = [16, 32, 64]\n",
    "\n",
    "    best_acc = 0\n",
    "    best_params = None\n",
    "\n",
    "    for d in depths:\n",
    "        for f in filters:\n",
    "            print(f\"\\nПробую depth={d}, filters={f}\")\n",
    "            model = build_cnn_model(depth=d, filters=f)\n",
    "            history = model.fit(\n",
    "                X_train, y_train_oh,\n",
    "                validation_data=(X_val, y_val_oh),\n",
    "                epochs=3, batch_size=64, verbose=0\n",
    "            )\n",
    "            acc = history.history[\"val_accuracy\"][-1]\n",
    "\n",
    "            print(f\"val_accuracy = {acc:.4f}\")\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = (d, f)\n",
    "\n",
    "    print(\"\\nЛучшие параметры:\", best_params, \"Acc =\", best_acc)\n",
    "    return best_params\n",
    "\n",
    "\n",
    "best_depth, best_filters = grid_search()\n",
    "\n",
    "\n",
    "# 5. АНАЛИЗ ВЛИЯНИЯ ГЛУБИНЫ СЕТИ НА КАЧЕСТВО\n",
    "depths_to_test = [1, 2, 3, 4]\n",
    "test_acc_results = []\n",
    "\n",
    "for d in depths_to_test:\n",
    "    print(f\"\\nТестирую глубину depth={d}\")\n",
    "    model = build_cnn_model(depth=d, filters=32)\n",
    "    model.fit(\n",
    "        X_train, y_train_oh,\n",
    "        validation_data=(X_val, y_val_oh),\n",
    "        epochs=3, batch_size=64, verbose=0\n",
    "    )\n",
    "    loss, acc = model.evaluate(X_test, y_test_oh, verbose=0)\n",
    "    test_acc_results.append(acc)\n",
    "    print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "# График зависимости точности от глубины\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(depths_to_test, test_acc_results, marker=\"o\")\n",
    "plt.title(\"Влияние глубины CNN на качество (Fashion-MNIST)\")\n",
    "plt.xlabel(\"Глубина (число Conv-блоков)\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-3-description",
   "metadata": {},
   "source": [
    "### Задание 3: Визуализация работы CNN\n",
    "\n",
    "**Задача:** Исследовать внутренние представления CNN и визуализировать feature maps\n",
    "\n",
    "**Подзадачи:**\n",
    "1. Создайте CNN модель с возможностью извлечения промежуточных представлений\n",
    "2. Визуализируйте feature maps разных слоев\n",
    "3. Проанализируйте, какие признаки извлекают разные слои сети\n",
    "4. Исследуйте влияние различных функций активации\n",
    "5. Сравните feature maps для разных классов данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assignment-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 1. ЗАГРУЗКА ДАТАСЕТА\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# нормализация\n",
    "X_train = X_train.astype(\"float32\")/255.0\n",
    "X_test = X_test.astype(\"float32\")/255.0\n",
    "\n",
    "# добавляем канал\n",
    "X_train = np.expand_dims(X_train, -1)   # (60000, 28, 28, 1)\n",
    "X_test  = np.expand_dims(X_test, -1)\n",
    "\n",
    "# 2. СОЗДАНИЕ CNN МОДЕЛИ (с возможностью извлечения feature maps)\n",
    "def build_visual_cnn(activation=\"relu\"):\n",
    "    inputs = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = layers.Conv2D(32, (3,3), activation=activation, padding=\"same\")(inputs)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3,3), activation=activation, padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3,3), activation=activation, padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=activation)(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_visual_cnn()\n",
    "model.summary()\n",
    "\n",
    "# быстрое обучение модели\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.1)\n",
    "\n",
    "\n",
    "# 3. СОЗДАНИЕ МОДЕЛИ ДЛЯ ИЗВЛЕЧЕНИЯ ПРОМЕЖУТОЧНЫХ ПРЕДСТАВЛЕНИЙ\n",
    "\n",
    "# берём выходы всех сверточных слоёв\n",
    "layer_outputs = [layer.output for layer in model.layers if \"conv\" in layer.name]\n",
    "\n",
    "# модель, которая возвращает список feature maps\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "\n",
    "# 4. ВИЗУАЛИЗАЦИЯ FEATURE MAPS\n",
    "def plot_feature_maps(img, feature_maps, layer_names):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, fmap in enumerate(feature_maps):\n",
    "        fmap = np.squeeze(fmap)  # remove batch dim\n",
    "        num_features = fmap.shape[-1]\n",
    "        size = fmap.shape[0]\n",
    "\n",
    "        # показываем первые 8 карт признаков\n",
    "        display_maps = min(8, num_features)\n",
    "\n",
    "        for j in range(display_maps):\n",
    "            plt.subplot(len(feature_maps), display_maps, i*display_maps + j + 1)\n",
    "            plt.imshow(fmap[:, :, j], cmap=\"viridis\")\n",
    "            plt.axis(\"off\")\n",
    "            if j == 0:\n",
    "                plt.ylabel(layer_names[i], fontsize=10)\n",
    "\n",
    "    plt.suptitle(\"Feature maps разных сверточных слоев\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# пример изображения\n",
    "img = X_test[0:1]\n",
    "true_label = y_test[0]\n",
    "\n",
    "feature_maps = activation_model.predict(img)\n",
    "layer_names = [layer.name for layer in model.layers if \"conv\" in layer.name]\n",
    "\n",
    "plot_feature_maps(img, feature_maps, layer_names)\n",
    "\n",
    "# 5. АНАЛИЗ: какие признаки извлекают слои?\n",
    "\n",
    "\n",
    "print(\"\\nАНАЛИЗ:\")\n",
    "print(\"Слой 1 — выявляет простые границы и текстуры.\")\n",
    "print(\"Слой 2 — формирует более сложные паттерны (кривые, углы).\")\n",
    "print(\"Слой 3 — выделяет высокоуровневые структуры (части объектов).\")\n",
    "\n",
    "\n",
    "# 6. ИССЛЕДОВАНИЕ РАЗНЫХ ФУНКЦИЙ АКТИВАЦИИ\n",
    "\n",
    "activations = [\"relu\", \"tanh\", \"sigmoid\"]\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "    print(f\"\\n=== Обучаю CNN с активацией {act} ===\")\n",
    "    m = build_visual_cnn(activation=act)\n",
    "    history = m.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.1, verbose=0)\n",
    "    loss, acc = m.evaluate(X_test, y_test, verbose=0)\n",
    "    activation_results[act] = acc\n",
    "    print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nСравнение функций активации:\")\n",
    "print(activation_results)\n",
    "\n",
    "# 7. СРАВНЕНИЕ FEATURE MAPS ДЛЯ РАЗНЫХ КЛАССОВ\n",
    "def compare_classes(class1, class2):\n",
    "    idx1 = np.where(y_test == class1)[0][0]\n",
    "    idx2 = np.where(y_test == class2)[0][0]\n",
    "\n",
    "    img1 = X_test[idx1:idx1+1]\n",
    "    img2 = X_test[idx2:idx2+1]\n",
    "\n",
    "    fm1 = activation_model.predict(img1)\n",
    "    fm2 = activation_model.predict(img2)\n",
    "\n",
    "    print(f\"\\nСравнение feature maps классов {class1} и {class2}\")\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "\n",
    "    # отображаем по 1 карте на слой\n",
    "    for i, fmap in enumerate(fm1):\n",
    "        plt.subplot(2, len(fm1), i+1)\n",
    "        plt.imshow(np.squeeze(fmap)[..., 0], cmap=\"inferno\")\n",
    "        plt.title(f\"{layer_names[i]} class {class1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    for i, fmap in enumerate(fm2):\n",
    "        plt.subplot(2, len(fm2), len(fm1) + i + 1)\n",
    "        plt.imshow(np.squeeze(fmap)[..., 0], cmap=\"inferno\")\n",
    "        plt.title(f\"{layer_names[i]} class {class2}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# пример: сравним класс 0 (\"T-shirt\") и 9 (\"boot\")\n",
    "compare_classes(0, 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-4-description",
   "metadata": {},
   "source": [
    "### Задание 4: Сравнение различных подходов к обработке данных\n",
    "\n",
    "**Задача:** Сравнить эффективность CNN с другими архитектурами нейронных сетей\n",
    "\n",
    "**Подзадачи:**\n",
    "1. Реализуйте RNN/LSTM для последовательных данных\n",
    "2. Создайте автокодировщик для обучения без учителя\n",
    "3. Сравните CNN с традиционными методами машинного обучения\n",
    "4. Проанализируйте преимущества и недостатки каждого подхода\n",
    "5. Определите оптимальные области применения для каждого метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assignment-4-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 19:18:30.010209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/mac/Documents/Intilect/Intelligence/Practical work №3/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes NN: (49000, 28, 28, 1) (7000, 28, 28, 1) (14000, 28, 28, 1)\n",
      "Classical data shape (PCA): (15000, 100)\n",
      "\n",
      "=> Обучаем LSTM (используем 28 timesteps x 28 features)\n",
      "Epoch 1/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 99ms/step - accuracy: 0.6516 - loss: 0.9914 - val_accuracy: 0.8070 - val_loss: 0.5118\n",
      "Epoch 2/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 104ms/step - accuracy: 0.8271 - loss: 0.4717 - val_accuracy: 0.8453 - val_loss: 0.4231\n",
      "Epoch 3/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 108ms/step - accuracy: 0.8455 - loss: 0.4091 - val_accuracy: 0.8484 - val_loss: 0.4036\n",
      "Epoch 4/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 115ms/step - accuracy: 0.8640 - loss: 0.3674 - val_accuracy: 0.8636 - val_loss: 0.3658\n",
      "Epoch 5/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3487 - val_accuracy: 0.8670 - val_loss: 0.3702\n",
      "Epoch 6/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 131ms/step - accuracy: 0.8745 - loss: 0.3388 - val_accuracy: 0.8686 - val_loss: 0.3497\n",
      "Epoch 7/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 112ms/step - accuracy: 0.8840 - loss: 0.3132 - val_accuracy: 0.8821 - val_loss: 0.3300\n",
      "Epoch 8/8\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 115ms/step - accuracy: 0.8884 - loss: 0.2999 - val_accuracy: 0.8834 - val_loss: 0.3155\n",
      "LSTM training time: 346.5 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras import layers, models, callbacks, losses\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X = np.concatenate([X_train_full, X_test], axis=0)\n",
    "y = np.concatenate([y_train_full, y_test], axis=0)\n",
    "\n",
    "# небольшая подвыборка для традиционных методов (иначе SVM будет долго)\n",
    "SUBSET_FOR_CLASSICAL = 15000  # оставь None для полного набора (но будет медленнее)\n",
    "if SUBSET_FOR_CLASSICAL:\n",
    "    idx = np.random.RandomState(42).choice(len(X), size=SUBSET_FOR_CLASSICAL, replace=False)\n",
    "    X_small = X[idx]\n",
    "    y_small = y[idx]\n",
    "else:\n",
    "    X_small = X\n",
    "    y_small = y\n",
    "\n",
    "# нормализация для нейросетей\n",
    "X_nn = X.astype('float32') / 255.0\n",
    "X_nn = np.expand_dims(X_nn, -1)  # (N,28,28,1)\n",
    "\n",
    "# разделение на train/val/test для нейросетей\n",
    "X_train, X_test_nn, y_train, y_test_nn = train_test_split(X_nn, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42, stratify=y_train)\n",
    "# итог: ~60% train, 10% val, 30% test\n",
    "\n",
    "print(\"Shapes NN:\", X_train.shape, X_val.shape, X_test_nn.shape)\n",
    "\n",
    "# для classical ML: плоский вектор + стандартизация / PCA опционально\n",
    "X_small_flat = X_small.reshape(len(X_small), -1).astype('float32') / 255.0\n",
    "scaler = StandardScaler()\n",
    "X_small_scaled = scaler.fit_transform(X_small_flat)\n",
    "\n",
    "# PCA для уменьшения размерности (ускорение SVM/RF) — сохраняем 100 компонент\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_small_pca = pca.fit_transform(X_small_scaled)\n",
    "print(\"Classical data shape (PCA):\", X_small_pca.shape)\n",
    "\n",
    "\n",
    "# 1. RNN / LSTM (изображение -> последовательность)\n",
    "# Подход: каждую строку 28 пикселей считаем timestep, размерность 28 (features). Последовательность длины 28.\n",
    "X_train_seq = X_train.squeeze(-1)  # (N,28,28)\n",
    "X_val_seq = X_val.squeeze(-1)\n",
    "X_test_seq = X_test_nn.squeeze(-1)\n",
    "\n",
    "# LSTM требует shape (samples, timesteps, features) => (N,28,28)\n",
    "def build_lstm_model(hidden_units=64):\n",
    "    inp = layers.Input(shape=(28, 28))\n",
    "    x = layers.Masking()(inp)  # не обязательно, но оставим\n",
    "    x = layers.LSTM(hidden_units, return_sequences=False)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    out = layers.Dense(10, activation='softmax')(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\n=> Обучаем LSTM (используем 28 timesteps x 28 features)\")\n",
    "lstm = build_lstm_model(128)\n",
    "start = time.time()\n",
    "history_lstm = lstm.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val), epochs=8, batch_size=128, verbose=1)\n",
    "lstm_time = time.time() - start\n",
    "print(\"LSTM training time: %.1f s\" % lstm_time)\n",
    "loss, acc = lstm.evaluate(X_test_seq, y_test_nn, verbose=0)\n",
    "print(\"LSTM test acc:\", acc)\n",
    "\n",
    "# 2. Автокодировщик (unsupervised)\n",
    "# Преобразуем в плоский вектор для простого dense автокодировщика или используем сверточный автокодировщик.\n",
    "X_ae_train = X_train  # (N,28,28,1)\n",
    "X_ae_val = X_val\n",
    "X_ae_test = X_test_nn\n",
    "\n",
    "def build_conv_autoencoder(latent_dim=64):\n",
    "    inp = layers.Input(shape=(28,28,1))\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inp)\n",
    "    x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    encoded = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "\n",
    "    # decoder\n",
    "    x = layers.Dense(7*7*64, activation='relu')(encoded)\n",
    "    x = layers.Reshape((7,7,64))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = layers.UpSampling2D((2,2))(x)\n",
    "    x = layers.Conv2DTranspose(32, (3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = layers.UpSampling2D((2,2))(x)\n",
    "    decoded = layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    ae = models.Model(inp, decoded)\n",
    "    encoder = models.Model(inp, encoded)  # отдельно encoder для получения латентных векторов\n",
    "    ae.compile(optimizer='adam', loss='mse')\n",
    "    return ae, encoder\n",
    "\n",
    "print(\"\\n=> Обучаем сверточный автокодировщик\")\n",
    "ae, encoder = build_conv_autoencoder(latent_dim=64)\n",
    "ae.fit(X_ae_train, X_ae_train, validation_data=(X_ae_val, X_ae_val), epochs=15, batch_size=256, verbose=1)\n",
    "# получаем латентные представления для теста\n",
    "Z_test = encoder.predict(X_ae_test)\n",
    "print(\"Latent shape:\", Z_test.shape)\n",
    "\n",
    "# Визуализация примеров реконструкции\n",
    "n = 6\n",
    "recon = ae.predict(X_ae_test[:n])\n",
    "plt.figure(figsize=(10,4))\n",
    "for i in range(n):\n",
    "    plt.subplot(2,n,i+1)\n",
    "    plt.imshow(X_ae_test[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,n,n+i+1)\n",
    "    plt.imshow(recon[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Оригинал (верх) vs Реконструкция (низ)\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Сравнение CNN с традиционными методами\n",
    "# 3a: CNN (сверточная модель)\n",
    "def build_cnn_small():\n",
    "    inp = layers.Input(shape=(28,28,1))\n",
    "    x = layers.Conv2D(32,(3,3),activation='relu',padding='same')(inp)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Conv2D(64,(3,3),activation='relu',padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128,activation='relu')(x)\n",
    "    out = layers.Dense(10,activation='softmax')(x)\n",
    "    model = models.Model(inp,out)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\n=> Обучаем базовый CNN\")\n",
    "cnn = build_cnn_small()\n",
    "start = time.time()\n",
    "cnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=6, batch_size=128, verbose=1)\n",
    "cnn_time = time.time() - start\n",
    "print(\"CNN training time: %.1f s\" % cnn_time)\n",
    "loss, cnn_acc = cnn.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "print(\"CNN test acc:\", cnn_acc)\n",
    "\n",
    "# 3b: Традиционные методы (RandomForest, LogisticRegression, SVM) на PCA-признаках\n",
    "print(\"\\n=> Обучаем классические методы на PCA-данных (subset)\")\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_small_pca, y_small, test_size=0.2, random_state=42, stratify=y_small)\n",
    "\n",
    "# RandomForest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "start = time.time()\n",
    "rf.fit(Xc_train, yc_train)\n",
    "rf_time = time.time() - start\n",
    "rf_acc = rf.score(Xc_test, yc_test)\n",
    "print(\"RF time: %.1f s, acc: %.4f\" % (rf_time, rf_acc))\n",
    "\n",
    "# LogisticRegression (multinomial)\n",
    "lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga', n_jobs=-1)\n",
    "start = time.time()\n",
    "lr.fit(Xc_train, yc_train)\n",
    "lr_time = time.time() - start\n",
    "lr_acc = lr.score(Xc_test, yc_test)\n",
    "print(\"LR time: %.1f s, acc: %.4f\" % (lr_time, lr_acc))\n",
    "\n",
    "# SVM (RBF) — может быть медленным\n",
    "svc = SVC(kernel='rbf', gamma='scale')\n",
    "start = time.time()\n",
    "svc.fit(Xc_train, yc_train)\n",
    "svc_time = time.time() - start\n",
    "svc_acc = svc.score(Xc_test, yc_test)\n",
    "print(\"SVC time: %.1f s, acc: %.4f\" % (svc_time, svc_acc))\n",
    "\n",
    "\n",
    "# 4. Анализ преимуществ и недостатков (подытожим результаты)\n",
    "print(\"\\n=== Сводная таблица (точности) ===\")\n",
    "results = {\n",
    "    'Model': ['LSTM(seq)', 'CNN', 'Autoencoder+Linear', 'RandomForest', 'LogisticRegression', 'SVM-RBF'],\n",
    "    'Test accuracy': [\n",
    "        round(float(acc),4),                      # LSTM acc variable 'acc'\n",
    "        round(float(cnn_acc),4),\n",
    "        None,  # заполнится ниже: we'll train a linear classifier on AE latents\n",
    "        round(float(rf_acc),4),\n",
    "        round(float(lr_acc),4),\n",
    "        round(float(svc_acc),4)\n",
    "    ],\n",
    "    'Train time (s)': [round(lstm_time,1), round(cnn_time,1), None, round(rf_time,1), round(lr_time,1), round(svc_time,1)]\n",
    "}\n",
    "\n",
    "# 4a: Автокодировщик + линейный классификатор на латентных признаках\n",
    "# берем латентные вектора из encoder для подвыборки X_small\n",
    "print(\"\\n=> Обучаем LogisticRegression на латентных векторах AE (вся тестовая выборка)\")\n",
    "# получим латенты для небольшого поднабора (чтобы не было долго)\n",
    "sub_idx = np.random.RandomState(1).choice(len(X), size=10000, replace=False)\n",
    "X_sub = X[sub_idx].astype('float32') / 255.0\n",
    "X_sub = np.expand_dims(X_sub, -1)\n",
    "y_sub = y[sub_idx]\n",
    "\n",
    "Z_sub = encoder.predict(X_sub, batch_size=256)\n",
    "Z_train, Z_test_lat, yZ_train, yZ_test = train_test_split(Z_sub, y_sub, test_size=0.2, random_state=42, stratify=y_sub)\n",
    "\n",
    "lr_ae = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga', n_jobs=-1)\n",
    "start = time.time()\n",
    "lr_ae.fit(Z_train, yZ_train)\n",
    "lr_ae_time = time.time() - start\n",
    "lr_ae_acc = lr_ae.score(Z_test_lat, yZ_test)\n",
    "print(\"LogReg on AE-latent: time %.1f s acc %.4f\" % (lr_ae_time, lr_ae_acc))\n",
    "\n",
    "# добавляем в results\n",
    "results['Test accuracy'][2] = round(float(lr_ae_acc),4)\n",
    "results['Train time (s)'][2] = round(lr_ae_time,1)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# 5. Доп. оценки: confusion matrix и classification report для CNN и RF\n",
    "print(\"\\n=== Classification report: CNN (test) ===\")\n",
    "y_pred_cnn = np.argmax(cnn.predict(X_test_nn), axis=1)\n",
    "print(classification_report(y_test_nn, y_pred_cnn))\n",
    "\n",
    "print(\"\\n=== Confusion matrix: CNN ===\")\n",
    "print(confusion_matrix(y_test_nn, y_pred_cnn))\n",
    "\n",
    "print(\"\\n=== Classification report: RandomForest (on PCA subset test) ===\")\n",
    "yc_pred_rf = rf.predict(Xc_test)\n",
    "print(classification_report(yc_test, yc_pred_rf))\n",
    "\n",
    "\n",
    "# Сохраняем результаты в CSV\n",
    "df_results.to_csv(\"comparison_results.csv\", index=False)\n",
    "print(\"\\nSaved results to comparison_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-criteria",
   "metadata": {},
   "source": [
    "### Теоретические вопросы:\n",
    "\n",
    "1. В чем преимущества CNN перед полносвязными сетями для обработки структурированных данных?\n",
    "2. Какие типы данных наиболее подходят для анализа с помощью CNN?\n",
    "3. Как функция пулинга влияет на работу сверточной сети?\n",
    "4. В чем разница между 1D, 2D и 3D свертками?\n",
    "5. Какие методы регуляризации наиболее эффективны для CNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a4013",
   "metadata": {},
   "source": [
    "ВАШ ОТВЕТ НА ВОПРОСЫ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88660195",
   "metadata": {},
   "source": [
    "1. CNN используют локальные рецептивные поля, поэтому улавливают пространственные закономерности.\n",
    "Количество параметров значительно меньше, чем у полносвязных слоев.\n",
    "Есть параметрическое разделение весов (один фильтр используется по всему изображению).\n",
    "Лучше обобщают на данных с локальными паттернами (изображения, временные ряды).\n",
    "2. CNN используют локальные рецептивные поля, поэтому улавливают пространственные закономерности.\n",
    "Количество параметров значительно меньше, чем у полносвязных слоев.\n",
    "Есть параметрическое разделение весов (один фильтр используется по всему изображению).\n",
    "Лучше обобщают на данных с локальными паттернами (изображения, временные ряды).\n",
    "3. Изображения (2D, 3D).\n",
    "Временные ряды или последовательности (1D-CNN).\n",
    "Видео (3D-CNN).\n",
    "Любые данные с локальной структурой, например спектрограммы аудио.\n",
    "4. Уменьшает размерность активаций → снижает вычисления.\n",
    "Делает сеть более инвариантной к небольшим смещениям.\n",
    "Помогает выделять наиболее значимые признаки (например, max-pool берёт самое сильное локальное активирование).\n",
    "5. 1D свертка: применяется по одному измерению (время, последовательности).\n",
    "2D свертка: применяется по двум пространственным измерениям (изображения).\n",
    "3D свертка: применяется по трём измерениям (видео, объемные данные, MRI)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
